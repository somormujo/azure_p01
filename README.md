# Optimizing an ML Pipeline in Azure
## Test
## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
The dataset bank marketing is having financial and personal information about different customers that went through different campaings and subscribed or not a term deposit with the bank.

The goal is to create a model that predicts (based on the data) whether the customer will subscribe to the deposit with the bank.

The hyperparametrized model (Logistic Regression) achieved a maximum accuracy of 91.18%
The autoML model with Voting Ensemble achieved a maximum accuracy of 91.62%. It's also worth mentioning XBoostClassifier (MaxAbsScaler) with an accuracy of 91.52%.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The steps of the scikit-learn pipeline are the following:
1. Define the arguments for the Logistic Regression (--C for inverse regularization and --max_iter for the maximum number of iterations to converge to a solution)
    1. This information is coming from the notebook through a random parameter sampling.
    2. Regularization the options are 01, 1, 1.4
    3. Max iterations 60, 80, 100, 120, 140
2. Log the used parameters (coming from the notebook)
3. Load the dataset from Azure ML examples library
4. Clean up the data (encodings for the string values to numerical values)
5. Data is split into training and test sets (33% for the test set). Random_state set so next execution throw same results
6. Model is fitted and evaluated against accuracy (maximize)
7. An early termination policy is setup


**What are the benefits of the parameter sampler you chose?**
The random parameter sampling offers similar accuracy than the grid parameter sampling with a much lower computational cost. The functional difference is to random exploration of the parameters space vs exahustive exploration of the parameters space.

**What are the benefits of the early stopping policy you chose?**
Bandit policy is an **agressive** early termination policy based on slack criteria, frequency and delay interval to evaluate the termination (or not) of the algorithm.

* slack_factor (The ratio used to calculate the allowed distance from the best performing experiment run.) = 0.05
* evaluation_interval (The frequency for applying the policy) = 5


## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
Auto ML creates and fit different machine learning models with the same trainning data and compares how well they fit. In our case the comparision is done over the accuracy (maximize).

31 models were run during the AutoML evaluation:
<pre>
ITER   PIPELINE                                       DURATION            METRIC      BEST
    0   MaxAbsScaler LightGBM                          0:00:12             0.9147    0.9147
    1   MaxAbsScaler XGBoostClassifier                 0:00:15             0.9152    0.9152
    2   MaxAbsScaler ExtremeRandomTrees                0:00:13             0.7311    0.9152
    3   SparseNormalizer XGBoostClassifier             0:00:12             0.9049    0.9152
    4   MaxAbsScaler LightGBM                          0:00:10             0.9114    0.9152
    5   MaxAbsScaler LightGBM                          0:00:10             0.8881    0.9152
    6   StandardScalerWrapper XGBoostClassifier        0:00:12             0.9056    0.9152
    7   MaxAbsScaler LogisticRegression                0:00:13             0.9086    0.9152
    8   StandardScalerWrapper ExtremeRandomTrees       0:00:10             0.8880    0.9152
    9   StandardScalerWrapper XGBoostClassifier        0:00:11             0.9073    0.9152
   10   SparseNormalizer LightGBM                      0:00:10             0.9005    0.9152
   11   StandardScalerWrapper XGBoostClassifier        0:00:11             0.9083    0.9152
   12   MaxAbsScaler LogisticRegression                0:00:13             0.9085    0.9152
   13   MaxAbsScaler SGD                               0:00:10             0.8508    0.9152
   14   StandardScalerWrapper XGBoostClassifier        0:00:12             0.9133    0.9152
   15   SparseNormalizer RandomForest                  0:00:21             0.7969    0.9152
   16   StandardScalerWrapper LogisticRegression       0:00:11             0.9027    0.9152
   17   StandardScalerWrapper RandomForest             0:00:14             0.9005    0.9152
   18   StandardScalerWrapper XGBoostClassifier        0:00:14             0.9139    0.9152
   19   TruncatedSVDWrapper RandomForest               0:01:30             0.8230    0.9152
   20   TruncatedSVDWrapper RandomForest               0:00:16             0.8359    0.9152
   21   StandardScalerWrapper XGBoostClassifier        0:00:31             0.9105    0.9152
   22   StandardScalerWrapper LightGBM                 0:00:32             0.9070    0.9152
   23   StandardScalerWrapper XGBoostClassifier        0:00:53             0.9076    0.9152
   24   MaxAbsScaler LightGBM                          0:00:29             0.8880    0.9152
   25   TruncatedSVDWrapper XGBoostClassifier          0:00:31             0.8880    0.9152
   26   StandardScalerWrapper LightGBM                 0:00:29             0.8924    0.9152
   27   StandardScalerWrapper XGBoostClassifier        0:00:52             0.8927    0.9152
   28   SparseNormalizer LightGBM                      0:00:32             0.9057    0.9152
   29   StandardScalerWrapper XGBoostClassifier        0:01:19             0.9105    0.9152
   30    VotingEnsemble                                0:00:50             0.9162    0.9162
</pre>

The most accurate was VotingEnsemble, and an honorable mention is XGBoostClassifier (MaxAbsScaler)

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
### Summary
Both models are having a very similar performance:
* Logistic Regression is having 91.18%
* AutoML (Voting Ensemble / XBoostClassifier) are having 91.62% and 91.52%
The Hyperparameter method allows us to improve the performance of a single model by automatically tweaking the parameters of the same using Random or Grid sampling of the parameters space.
The AutoML method allows us to explore the performance of several different models (with default parameters) and choose the best one for the task at hand. Probably after selecting the best model is a good idea to find the best parameters using the hyperparameter method.

### HyperParameter Logistic Regression
Some images about the performance of the logistic regression hyperparameters.
![Accuracy vs Max Iterations](https://github.com/somormujo/azure_p01/blob/main/Hyper-AccuracyvsIter.png)
![Accuracy of the different experiments](https://github.com/somormujo/azure_p01/blob/main/Hyper-Experiments.png)

### AutoML Voting Ensemble
Some images about the performance of the voting ensemble.
![Main Contributing Features](https://github.com/somormujo/azure_p01/blob/main/AutoML-Features.png)
![Precision and ROC](https://github.com/somormujo/azure_p01/blob/main/AutoML-PrecisionROC.png)
![Gain](https://github.com/somormujo/azure_p01/blob/main/AutoML-Gain.png)

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
As mentioned before it would be a good idea to execute hyperparameters in the best 2 AutoML models (XGBoostClassifier and Voting Ensemble)

In the training data, there are 3600 persons that **agreed** to the deposit, while there are +29000 persons that did **NOT agree** to the deposit. This creates a training data that is not very evenly distributed. A dumb classifier that always says person will not agree to the deposit is having a wooping 88.79% of accuracy (less than 3% away of the best model). Here we can see that machine learning models do not perform well when the data is not balanced.

Ideas to improve this:
* Get more samples of people agreeing to the deposit
    * Synthetically (generating the data ourselves)
    * Manually (getting more data from other sources)
* Remove people that did not agree to the deposit
    * This will make the training set smaller
    * But it will be more evenly distributed
